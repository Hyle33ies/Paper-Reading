## Quick Summary

| Year | Titile                                                       | Venue           | Paper Link                                                   | TL;DR                                                        | Links                                                        |
| ---- | ------------------------------------------------------------ | --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2019 | PRADA: Protecting Against DNN Model Stealing Attacks         | EuroS&P         | [IEEE](https://ieeexplore.ieee.org/document/8806737/?arnumber=8806737&tag=1) | Defending black-box model extraction                         |                                                              |
| 2019 | Deep Leakage from Gradients                                  | NeurlPS         | [NeurlPS](https://papers.nips.cc/paper_files/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html), [ArXiv](https://arxiv.org/pdf/1906.08935) | Model inversions from gradients                              |                                                              |
| 2021 | Extracting Training Data from Large Language Models          | USENIX Security | [Usenix](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting), [ArXiv](https://arxiv.org/pdf/2012.07805) | Training data extraction from LLMs                           | [Code](https://github.com/ftramer/LM_Memorization)           |
| 2021 | Label-Only Membership Inference Attacks                      | ICML            | [PMLR](https://proceedings.mlr.press/v139/choquette-choo21a.html) | Label-only MIA by multiple queries                           | [Code](https://github.com/cchoquette/  membership-inference.) |
| 2022 | Teacher Model Fingerprinting Attacks Against Transfer Learning | USENIX Security | [Usenix](https://www.usenix.org/conference/usenixsecurity22/presentation/chen-yufei) | Finding model origins from transferred models                |                                                              |
| 2022 | Enhanced Membership Inference Attacks against Machine Learning Models | CCS             | [ACM](https://dl.acm.org/doi/10.1145/3548606.3560675)        | Membership inference attacks with multiple scenarios         |                                                              |
| 2023 | Membership Inference Attacks against Language Models via Neighbourhood Comparison | ACL             | [ArXiv](http://arxiv.org/abs/2305.18462), [ACL](https://aclanthology.org/2023.findings-acl.719/) | Novel reference-based membership inference attacks for LLMs with artificially crafted neighbors | [Code](https://github.com/mireshghallah/neighborhood-curvature-mia) |
| 2024 | Certifiable Black-Box Attacks with Randomized Adversarial Examples: Breaking Defenses with Provable Confidence | CCS             | [ArXiv](http://arxiv.org/abs/2304.04343)                     | Certified black-box adversarial examples breaking defenses   |                                                              |
| 2024 | Do Membership Inference Attacks Work on Large Language Models? | COLM            | [ArXiv](http://arxiv.org/abs/2402.07841)                     | Empirical facts on why MIAs don't work for LLMs              | [Code](https://github.com/iamgroot42/mimir)                  |
| 2024 | SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice | USENIX Security | [Usenix](https://www.usenix.org/conference/usenixsecurity24/presentation/nayan) | On-device white-box model stealing                           | [Appendix](https://www.usenix.org/system/files/usenixsecurity24-appendix-nayan.pdf), [Code](https://github.com/sys-ris3/ML_Extraction_Sok/tree/0d19edab5b5bd4bad4562543f4c1457be3c30852) |
| 2024 | How Does a Deep Learning Model Architecture Impact Its Privacy? A Comprehensive Study of Privacy Attacks on CNNs and Transformers | USENIX Security | [Usenix](https://www.usenix.org/conference/usenixsecurity24/presentation/zhang-guangsheng), [ArXiv](https://arxiv.org/abs/2210.11049) | Empirical results on why transformers are more vulnerable to three privacy attacks |                                                              |
| 2024 | Min-K%++ Improved Baseline for Detecting Pre-Training Data from Large Language Models | \ (ArXiv 2404)  | [ArXiv](http://arxiv.org/abs/2404.02936)                     | Novel MIA on LLMs that proposes a new calibrated score function | [Code](\url{https://github.com/zjysteven/mink-plus-plus}), [ICLR Review](https://openreview.net/forum?id=ZGkfoufDaU) |
| 2024 | Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large Language Models with Ex-Post Dataset Construction | \ (ArXiv 2408)  | [ArXiv](http://arxiv.org/abs/2408.05968)                     | Constructing datasets without inherent bias invalidates most SOTA MIAs | [Code](https://github.com/ceichler/MIA-bias-removal)         |
| 2024 | Detecting Training Data of Large Language Models via Expectation Maximization | \ (ArXiv 2410)  | [ArXiv](http://arxiv.org/abs/2410.07582)                     | A optimized prefix scores-based MIA for LLMs; A new benchmark for detecting pre-training data. | [Code](https://github.com/gyuwankim/em-mia)                  |
| 2024 | Did the Neurons Read your Book?  Document-level Membership Inference for Large Language Models | USENIX Security | [ArXiv](http://arxiv.org/abs/2310.15007)                     | MIA at LLMs at the document level                            | [Code](https://github.com/computationalprivacy/document-level-membership-inference) |
| 2024 | Scaling Up Membership Inference When and How Attacks Succeed on Large Language Models | \ (ArXiv2411)   | [ArXiv](http://arxiv.org/abs/2411.00154)                     | Scaling up the MIA to document and even dataset level and succeed | [Code](https://github.com/parameterlab/mia-scaling)          |
| 2024 | Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration | NeurlPS         | [OpenReview](https://openreview.net/pdf?id=PAWQvrForJ)       | Constructing reference models with LLM distillation          | [Code](https://github.com/tsinghua-fib-lab/ANeurIPS2024_SPV-MIA) |
| 2025 | Adversarial ML Problems Are Getting Harder to Solve and to Evaluate | \ (ArXiv2502)   | [ArXiv](http://arxiv.org/abs/2502.02260)                     | Position Paper on Adversarial Machine Learning               |                                                              |
| 2025 | Does Training with Synthetic Data Truly Protect Privacy?     | ICLR            | [ArXiv](https://arxiv.org/pdf/2502.12976), [OpenReview](https://openreview.net/forum?id=C8niXBHjfO) | Does training with synthetic data truly protect privacy? No  | [Code](https://github.com/yunpeng-zhao/ syndata-privacy)     |

