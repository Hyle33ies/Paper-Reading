## Quick Summary

| Year | Titile                                                       | Venue           | Paper Link                                                   | TL;DR                                                        |
| ---- | ------------------------------------------------------------ | --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2019 | PRADA: Protecting Against DNN Model Stealing Attacks         | EuroS&P         | [IEEE](https://ieeexplore.ieee.org/document/8806737/?arnumber=8806737&tag=1) | Defending black-box model extraction                         |
| 2019 | Deep Leakage from Gradients                                  | NeurlPS         | [NeurlPS](https://papers.nips.cc/paper_files/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html), [ArXiv](https://arxiv.org/pdf/1906.08935) | Model inversions from gradients                              |
| 2021 | Extracting Training Data from Large Language Models          | USENIX Security | [Usenix](https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting), [ArXiv](https://arxiv.org/pdf/2012.07805) | Training data extraction from LLMs                           |
| 2022 | Teacher Model Fingerprinting Attacks Against Transfer Learning | USENIX Security | [Usenix](https://www.usenix.org/conference/usenixsecurity22/presentation/chen-yufei) | Finding model origins from transferred models                |
| 2022 | Enhanced Membership Inference Attacks against Machine Learning Models | CCS             | [ACM](https://dl.acm.org/doi/10.1145/3548606.3560675)        | Membership inference attacks with multiple scenarios         |
| 2023 | Membership Inference Attacks against Language Models via Neighbourhood Comparison | ACL             | [ArXiv](http://arxiv.org/abs/2305.18462), [ACL](https://aclanthology.org/2023.findings-acl.719/) | Reference-based membership inference attacks for LLMs        |
| 2024 | Certifiable Black-Box Attacks with Randomized Adversarial Examples: Breaking Defenses with Provable Confidence | CCS             | [ArXiv](http://arxiv.org/abs/2304.04343)                     | Certified black-box adversarial examples breaking defenses   |
| 2024 | Do Membership Inference Attacks Work on Large Language Models? | COLM            | [ArXiv](http://arxiv.org/abs/2402.07841)                     | Empirical facts on why MIAs don't work for LLMs              |
| 2024 | SoK: All You Need to Know About On-Device ML Model Extraction - The Gap Between Research and Practice | USENIX Security | [Usenix](https://www.usenix.org/conference/usenixsecurity24/presentation/nayan) | On-device white-box model stealing                           |
| 2024 | How Does a Deep Learning Model Architecture Impact Its Privacy? A Comprehensive Study of Privacy Attacks on CNNs and Transformers | USENIX Security | [Usenix](https://www.usenix.org/conference/usenixsecurity24/presentation/zhang-guangsheng), [ArXiv](https://arxiv.org/abs/2210.11049) | Empirical results on why transformers are more vulnerable to three privacy attacks |
| 2024 | Min-K%++ Improved Baseline for Detecting Pre-Training Data from Large Language Models | \ (ArXiv 2404)  | [ArXiv](http://arxiv.org/abs/2404.02936)                     | Novel MIA on LLMs that proposes a new calibrated score function |
| 2024 | Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large Language Models with Ex-Post Dataset Construction | \ (ArXiv 2408)  | [ArXiv](http://arxiv.org/abs/2408.05968)                     | Constructing datasets without inherent bias invalidates most SOTA MIAs |
| 2024 | Detecting Training Data of Large Language Models via Expectation Maximization | \ (ArXiv 2410)  | [ArXiv](http://arxiv.org/abs/2410.07582)                     | A optimized prefix scores-based MIA for LLMs; A new benchmark for detecting pre-training data. |
| 2024 | Did the Neurons Read your Book?  Document-level Membership Inference for Large Language Models | USENIX Security | [ArXiv](http://arxiv.org/abs/2310.15007)                     | MIA at LLMs at the document level                            |

